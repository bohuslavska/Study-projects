{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a1ce4a",
   "metadata": {},
   "source": [
    "## About the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08b46e8",
   "metadata": {},
   "source": [
    ">The project was completed as a part of the Natural Language Processing course from JetBrains Academy.\n",
    "\n",
    ">The program takes two parameters from the user input (the number of pages and the type of article). Then it goes all over the website and saves every article of the specified type to a separate .txt file on your computer. The number of pages from the user input determines how many pages the program should check for the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c526f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01612cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "News\n"
     ]
    }
   ],
   "source": [
    "user_input_number = int(input())\n",
    "user_input_type = str(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74cffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles saved to Page_1 folder\n",
      "Articles saved to Page_2 folder\n"
     ]
    }
   ],
   "source": [
    "#Iterating through the pages of the website\n",
    "\n",
    "for u in range(1, user_input_number+1): \n",
    "    \n",
    "    URL = f'https://www.nature.com/nature/articles?searchType=journalSearch&sort=PubDate&year=2020&page={u}'\n",
    "    content = requests.get(URL).text\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "    #Creating folders for the articles\n",
    "    \n",
    "    general_path = 'Page_' + str(u)\n",
    "    os.mkdir(general_path)\n",
    "    os.chdir(os.getcwd()+'/' + general_path)\n",
    "    \n",
    "    #Iterating through the list of articles on the page and scraping their names, types and links\n",
    "    \n",
    "    publications = soup.find_all('li', class_='app-article-list-row__item')   \n",
    "    for publication in publications: \n",
    "        title = publication.find('h3', class_= 'c-card__title').text\n",
    "        article_name = title.translate(str.maketrans('', '', string.punctuation)).replace(' ','_').strip()\n",
    "        kind = publication.find('span', class_='c-meta__type').text\n",
    "        prelink = publication.find('a', {'data-track-action':'view article'}).get('href')\n",
    "        article_link = 'https://www.nature.com'+ prelink\n",
    "        dict = {'name': article_name, 'type': kind, 'link':article_link}\n",
    "        \n",
    "        #Scraping texts of the articles\n",
    "\n",
    "        if user_input_type == dict['type']: \n",
    "            URL2 = dict['link']\n",
    "            response = requests.get(URL2).text\n",
    "            soup = BeautifulSoup(response, \"html.parser\")\n",
    "            text_article = soup.find('div', class_='c-article-body u-clearfix').text.strip()\n",
    "            \n",
    "            #Writing articles to the .txt files \n",
    "\n",
    "            ready_file = open(article_name + '.txt', 'w', encoding='utf-8')\n",
    "            ready_file.write(text_article)\n",
    "            ready_file.close()\n",
    "            \n",
    "    print(f\"Articles saved to {general_path} folder\")\n",
    "    os.chdir(os.path.split(os.getcwd())[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
