{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3ca580",
   "metadata": {},
   "source": [
    "# About the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df90032",
   "metadata": {},
   "source": [
    ">The project was completed as a part of the [Natural Language Processing course](https://hyperskill.org/tracks/10) on JetBrains Academy.\n",
    "\n",
    ">The aim of this project was to learn how to extract key terms from a collection of news stories. While doing this project, I found out how to\n",
    "\n",
    ">- read an XML file and extract the headers and the text;\n",
    ">- **tokenize** each text;\n",
    ">- **lemmatize** each word in each story;\n",
    ">- get rid of punctuation, stopwords, and non-nouns with the help of NLTK;\n",
    ">- count the **TF-IDF metric** for each word in all stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4807fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Parcing through an XML-file\n",
    "\n",
    "xml_path = \"/Users/katerynaboguslavska/Downloads/news.xml\"\n",
    "tree = etree.parse(xml_path)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "623581bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the headers and the text\n",
    "\n",
    "list_for_vectorizer = []\n",
    "list_of_names = []\n",
    "\n",
    "for tag in root[0].findall('news/value'):\n",
    "    \n",
    "    if tag.attrib['name'] == 'head':\n",
    "        list_of_names.append(tag.text)\n",
    "        \n",
    "    if tag.attrib['name'] == 'text':\n",
    "        \n",
    "        #Applying tokenization and lemmatization\n",
    "        \n",
    "        tknzd_text = word_tokenize(tag.text.lower())\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemma_text = [lemmatizer.lemmatize(w) for w in tknzd_text]\n",
    "        \n",
    "        #Getting rid of punctuation and stopwords\n",
    "        \n",
    "        without_punct = [word for word in lemma_text if word not in list(string.punctuation)]\n",
    "        without_stopwords = [word for word in without_punct if word not in stopwords.words('english')]\n",
    "        \n",
    "        #Applying part-of-speech tagging (POS-tagging) and choosing nouns\n",
    "        \n",
    "        pos_tag = [nltk.pos_tag([word]) for word in without_stopwords]\n",
    "        extr_nouns = [word[0][0] for word in pos_tag if word[0][1] == \"NN\"]\n",
    "        list_of_nouns = ' '.join(extr_nouns)\n",
    "        list_for_vectorizer.append(list_of_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18463f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying TfidfVectorizer for every word in all news stories\n",
    "        \n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(list_for_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "286f26dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain Disconnects During Sleep:\n",
      "sleep cortex consciousness tononi tm\n",
      "New Portuguese skull may be an early relative of Neandertals:\n",
      "skull fossil europe trait genus\n",
      "Living by the coast could improve mental health:\n",
      "health coast mental living household\n",
      "Did you knowingly commit a crime? Brain scans could tell:\n",
      "brain suitcase study security scenario\n",
      "Computer learns to detect skin cancer more accurately than doctors:\n",
      "dermatologist skin melanoma cnn lesion\n",
      "US economic growth stronger than expected despite weak demand:\n",
      "rate growth quarter economy investment\n",
      "Microsoft becomes third listed US firm to be valued at $1tn:\n",
      "microsoft share cloud market company\n",
      "Apple's Siri is a better rapper than you:\n",
      "siri rhyme smooth rizzo producer\n",
      "Netflix viewers like comedy for breakfast and drama at lunch:\n",
      "netflix day comedy viewer tv\n",
      "Loneliness May Make Quitting Smoking Even Tougher:\n",
      "smoking loneliness smoke quit lead\n"
     ]
    }
   ],
   "source": [
    "#Looping through each news story\n",
    "\n",
    "for x in range(0, len(list_of_names)):\n",
    "    \n",
    "    print(list_of_names[x] + ':')\n",
    "    \n",
    "    #Creating dataFrame with TF-IDF for every word in news story\n",
    "    \n",
    "    df = pd.DataFrame(tfidf_matrix[x].toarray().transpose(), index=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    #Sorting it according to the task instructions\n",
    "        \n",
    "    df_sorted_1 = df.sort_values(0, ascending = False)\n",
    "    df_sorted_2 = df_sorted_1.sort_index(ascending = False)\n",
    "        \n",
    "    #Picking the five best scoring words\n",
    "    \n",
    "    top_five = df_sorted_2.nlargest(5,0)\n",
    "    \n",
    "    #Extracting keywords\n",
    "    \n",
    "    keywords_in_dict = top_five.transpose().to_dict(orient = 'list')\n",
    "    keywords = ' '.join(list(keywords_in_dict.keys()))\n",
    "    print(keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
